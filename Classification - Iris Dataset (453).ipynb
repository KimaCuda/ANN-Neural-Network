{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP HOMEWORK 01**\n",
    "# IRIS CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Formula\n",
    "\\begin{equation*}\n",
    "Cost = (\\alpha^{(L)} - y)^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\alpha^{(L)} = \\sigma \\quad (w^{(L)} * \\alpha^{(L-1)} + b^{(L)})\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sigma = S(x) = \\frac{1} {1+e^{-x}}\n",
    "\\end{equation*}\n",
    "\n",
    "## Back-Propagation\n",
    "\\begin{equation*}\n",
    "\\bigtriangleup W_{h_{n}} = \\eta * \\delta_y * h_n \\\\\n",
    "\\bigtriangleup b_{y} = - \\eta * \\delta_y\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\bigtriangleup W_{h_{x}} = \\eta * \\delta_{h_{n}} * X_i \\\\\n",
    "\\bigtriangleup b_{h_{n}} = \\eta * \\delta_{h_{n}}\n",
    "\\end{equation*}\n",
    "\n",
    "**For All weight and biases update**\n",
    "\\begin{equation*}\n",
    "w_t = w + \\bigtriangleup w \\\\\n",
    "b_t = b + \\bigtriangleup b\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import copy\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# parameter\n",
    "learning_rate = 0.5\n",
    "nb_iteration = 1000\n",
    "\n",
    "# input, Hiddenlayer, Output\n",
    "#                    I  H  Y\n",
    "neuron_layer_list = [4, 9, 3]\n",
    "\n",
    "nb_hidden_layer = len(neuron_layer_list) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating random weight and bias\n",
    "def initiaterandom():\n",
    "    weight_matrix = []\n",
    "    bias_matrix = []\n",
    "    for i in range(len(neuron_layer_list) - 1):\n",
    "        random_bias_list = [rnd.uniform(0,1) for j in range(neuron_layer_list[i+1])]\n",
    "        bias_matrix.append(random_bias_list)\n",
    "        # for each neuron in spesific hidden layer\n",
    "        for j in range(neuron_layer_list[i+1]):\n",
    "            random_weight_list = [rnd.uniform(0,1) for k in range(neuron_layer_list[i])]\n",
    "            weight_matrix.append(random_weight_list)\n",
    "    return weight_matrix, bias_matrix\n",
    "\n",
    "def datasets():\n",
    "    # read iris dataset csv using pandas\n",
    "    data_matrix = pd.read_csv(\"Iris_Dataset.csv\")\n",
    "    # create output neuron\n",
    "    data_matrix['o1'] = 0\n",
    "    data_matrix['o2'] = 0\n",
    "    data_matrix['o3'] = 0\n",
    "    # set output value. iris-setosa[1,0,0], iris-versicolor[0,1,0], iris-virginica[0,0,1]\n",
    "    data_matrix.loc[data_matrix['Output'] == 'Iris-setosa', 'o1'] = 1\n",
    "    data_matrix.loc[data_matrix['Output'] == 'Iris-versicolor', 'o2'] = 1\n",
    "    data_matrix.loc[data_matrix['Output'] == 'Iris-virginica', 'o3'] = 1\n",
    "    # shuffle dataset\n",
    "    data_matrix = data_matrix.sample(frac=1).reset_index(drop=True)\n",
    "    # delete output column\n",
    "    del data_matrix['Output']\n",
    "    # convert dataframe to list\n",
    "    data_list = data_matrix.values.tolist()\n",
    "    # normalize\n",
    "    data_list2 = np.array(data_list)\n",
    "    data_list3 = normalize(data_list2[:,0:4]).tolist()\n",
    "    data_list4 = data_list2[:,4:].tolist()\n",
    "    data_list5 = []\n",
    "    for i in range(len(data_list3)):\n",
    "        data_list3[i].extend(data_list4[i])\n",
    "        data_list5.append(data_list3[i])\n",
    "    data_list6 = data_list5[0:120].copy()\n",
    "    data_list7 = data_list5[120:].copy()\n",
    "    return data_list6, data_list7\n",
    "\n",
    "def sigmoid_calc(z):\n",
    "    sigmoid = 1/(1+math.e**(-z))\n",
    "    return sigmoid\n",
    "\n",
    "def learningphase(dataset, w_matrix, b_matrix):\n",
    "    h_value_list = []\n",
    "    y_output_list = []\n",
    "    # LEARNING PHASE\n",
    "    # only 1 for this case\n",
    "    for k in range(nb_hidden_layer):\n",
    "        z_list = []\n",
    "        h_list = []\n",
    "        # for each of neuron is in the spesific hidden layer. k is refering n th to the layer. +1 because neuron layer list\n",
    "        # from input amount --> hidden layer --> output\n",
    "        for l in range(neuron_layer_list[k+1]):\n",
    "            # for each number of input coming to each neuron in the hidden layer. 0 refers to the input, because how\n",
    "            # many neuron input use is stored in first list of neuron layer list\n",
    "            sigma_w_a = 0\n",
    "            w = w_matrix[l]\n",
    "            for m in range(neuron_layer_list[0]):\n",
    "                sigma_w_a = sigma_w_a + (dataset[m] * w[m])\n",
    "            b = b_matrix[k]\n",
    "            z = sigma_w_a - b[l]\n",
    "            z_list.append(z)\n",
    "\n",
    "        # applying sigmoid function to z\n",
    "        for l in range(len(z_list)):\n",
    "            h = sigmoid_calc(z_list[l])\n",
    "            h_list.append(h)\n",
    "        h_value_list.append(h_list)\n",
    "\n",
    "    # for the number of neuron output have\n",
    "    for j in range(neuron_layer_list[-1]):\n",
    "        # number of input coming from the latest hidden layer to output neuron\n",
    "        # because list[-1] is output, the last hidden layer should be list[-2]\n",
    "        z_output = 0\n",
    "        for k in range(neuron_layer_list[-2]):\n",
    "            w = w_matrix[neuron_layer_list[-2]+j]\n",
    "            z_output = z_output + (h_value_list[-1][k]*w[k])\n",
    "        b = b_matrix[-1]\n",
    "        z_output = z_output - b[j]\n",
    "        y_output = sigmoid_calc(z_output)\n",
    "        y_output_list.append(y_output)\n",
    "\n",
    "    return h_value_list, y_output_list\n",
    "\n",
    "def reverselist(h_value_list, dataset, w_matrix):\n",
    "    h_value_list_reversed = h_value_list[::-1]\n",
    "    #h_value_list_reversed.append(dataset)\n",
    "    w_matrix_reversed = w_matrix[::-1]\n",
    "    return h_value_list_reversed, w_matrix_reversed\n",
    "\n",
    "def backpropagation(h_value_list_reversed, y_output_list, dataset, w_matrix_reversed, b_matrix):\n",
    "    # BACK PROPAGATION START FROM HERE\n",
    "    delta_y_list = []\n",
    "    delta_h_list = []\n",
    "    triangle_w_list = []\n",
    "    triangle_b_list = []\n",
    "    sigma_deltay_times_w = []\n",
    "    # for how many layer to do back propagation (neuron layer list minus input layer)\n",
    "    for h in range(len(neuron_layer_list) - 1):\n",
    "        last_layer_neuron = neuron_layer_list[-1 * (1 + h)]\n",
    "        second_last_layer_neuron = neuron_layer_list[-1 * (2+h)]\n",
    "        # if the layer is output\n",
    "        if h == 0: \n",
    "            triangle_b_temp = []\n",
    "            delta_y_times_w = []\n",
    "            # for each neuron in output.\n",
    "            for i in range(last_layer_neuron):\n",
    "                # find delta y and triangle bias output\n",
    "                delta_y = y_output_list[i] * (dataset[-3 + i]-y_output_list[i]) * (1-y_output_list[i])\n",
    "                delta_y_list.append(delta_y)\n",
    "                triangle_by = -1 * learning_rate * delta_y\n",
    "                triangle_b_temp.append(triangle_by)\n",
    "                triangle_whn_temp = []\n",
    "                # for all neuron that is in second last layer\n",
    "                for j in range(second_last_layer_neuron):\n",
    "                    # find delta w\n",
    "                    triangle_whn = learning_rate * delta_y * h_value_list_reversed[h][j]\n",
    "                    triangle_whn_temp.append(triangle_whn)\n",
    "                triangle_w_list.append(triangle_whn_temp)\n",
    "            triangle_b_list.append(triangle_b_temp)\n",
    "            triangle_w_list = triangle_w_list[::-1]\n",
    "\n",
    "            #find value sigma delta_y * weight\n",
    "            # for each neuron in hidden layer before output\n",
    "            for i in range(second_last_layer_neuron):\n",
    "                delta_temp = []\n",
    "                #for each neuron in output\n",
    "                for j in range(last_layer_neuron):\n",
    "                    a = delta_y_list[-1 - j] * w_matrix_reversed[j][i]\n",
    "                    delta_temp.append(a)\n",
    "                delta_y_times_w.append(delta_temp)\n",
    "            sigma_deltay_times_w.append(delta_y_times_w)\n",
    "\n",
    "        else:\n",
    "            triangle_b_temp = []\n",
    "            delta_h_temp = []\n",
    "            # number of neuron is in the target hidden layer\n",
    "            for i in range(last_layer_neuron):\n",
    "                # find delta h and triangle bias output\n",
    "                delta_h = h_value_list_reversed[-1 * (h)][-1 * (i+1)] * (1-h_value_list_reversed[-1 * (h)][-1 * (i+1)])*sum(sigma_deltay_times_w[h-1][-1*(i+1)])\n",
    "                delta_h_temp.append(delta_h)\n",
    "                triangle_bh = -1 * learning_rate * delta_h\n",
    "                triangle_b_temp.append(triangle_bh)\n",
    "                triangle_whn_temp = []\n",
    "                # for all neuron that is in second last layer\n",
    "                for j in range(second_last_layer_neuron):\n",
    "                    # find delta w\n",
    "                    triangle_whn = learning_rate * delta_h * dataset[j]\n",
    "                    triangle_whn_temp.append(triangle_whn)\n",
    "                triangle_w_list.append(triangle_whn_temp)\n",
    "            delta_h_temp = delta_h_temp[::-1]\n",
    "            delta_h_list.append(delta_h_temp)\n",
    "            triangle_b_temp = triangle_b_temp[::-1]\n",
    "            triangle_b_list.append(triangle_b_temp)\n",
    "    \n",
    "    return delta_y_list, delta_h_list, triangle_w_list, triangle_b_list, sigma_deltay_times_w\n",
    "\n",
    "def adjustweightandbias(w_matrix, b_matrix, triangle_w_list, triangle_b_list):\n",
    "    # reverse back triangle lists first\n",
    "    triangle_w_list_reversedback = triangle_w_list[::-1]\n",
    "    triangle_b_list_reversedback = triangle_b_list[::-1]\n",
    "    # for weight adjustment\n",
    "    new_w_matrix = []\n",
    "    for i in range(len(w_matrix)):\n",
    "        w = (np.array(w_matrix[i]) + np.array(triangle_w_list_reversedback[i])).tolist()\n",
    "        new_w_matrix.append(w)\n",
    "    # for bias adjustment\n",
    "    new_b_matrix = []\n",
    "    for i in range(len(b_matrix)):\n",
    "        b = (np.array(b_matrix[i]) + np.array(triangle_b_list_reversedback[i])).tolist()\n",
    "        new_b_matrix.append(b)\n",
    "\n",
    "    return new_w_matrix, new_b_matrix\n",
    "    \n",
    "def cost_calculation():\n",
    "    ## THIS SHOULD BE THE MAIN formula\n",
    "    w_matrix, b_matrix = initiaterandom()\n",
    "    dataset, training_set = datasets()\n",
    "    all_yj = []\n",
    "    error_list = []\n",
    "    #i = 0\n",
    "    #while error >= 0.1:\n",
    "    for i in range(nb_iteration):    \n",
    "        y_output_rmse = []\n",
    "        for j in range(len(dataset)):\n",
    "            x = dataset[j]\n",
    "            h_value_list, y_output_list = learningphase(x, w_matrix, b_matrix)\n",
    "            h_value_list_reversed, w_matrix_reversed = reverselist(h_value_list, x, w_matrix)\n",
    "            delta_y_list, delta_h_list, triangle_w_list, triangle_b_list, sigma_deltay_times_w =\\\n",
    "            backpropagation(h_value_list_reversed, y_output_list, x, w_matrix_reversed, b_matrix)\n",
    "            w_matrix, b_matrix = adjustweightandbias(w_matrix, b_matrix, triangle_w_list, triangle_b_list)\n",
    "            y_output_rmse.append(y_output_list)\n",
    "        all_yj.append(y_output_rmse)\n",
    "        # for each dataset\n",
    "        errors = []\n",
    "        for k in range(len(y_output_rmse)):\n",
    "            x = dataset[k]\n",
    "            # for each neuron output have\n",
    "            for l in range(neuron_layer_list[-1]):\n",
    "                error = ((y_output_rmse[k][l] - x[neuron_layer_list[0]+l])**2)\n",
    "                errors.append(error)       \n",
    "        errorss = math.sqrt(sum(errors) / (len(y_output_rmse)*neuron_layer_list[-1]))\n",
    "        error_list.append(errorss)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Iteration #%s. Error = %s' % (str(i+1), str(errorss)))\n",
    "    return w_matrix, b_matrix, dataset, training_set, error_list\n",
    "    \n",
    "def recall(dataset, w_matrix, b_matrix):\n",
    "    h_value_list = []\n",
    "    y_output_list = []\n",
    "    # LEARNING PHASE\n",
    "    # only 1 for this case\n",
    "    for k in range(nb_hidden_layer):\n",
    "        z_list = []\n",
    "        h_list = []\n",
    "        # for each of neuron is in the spesific hidden layer. k is refering n th to the layer. +1 because neuron layer list\n",
    "        # from input amount --> hidden layer --> output\n",
    "        for l in range(neuron_layer_list[k+1]):\n",
    "            # for each number of input coming to each neuron in the hidden layer. 0 refers to the input, because how\n",
    "            # many neuron input use is stored in first list of neuron layer list\n",
    "            sigma_w_a = 0\n",
    "            w = w_matrix[l]\n",
    "            for m in range(neuron_layer_list[0]):\n",
    "                sigma_w_a = sigma_w_a + (dataset[m] * w[m])\n",
    "            b = b_matrix[k]\n",
    "            z = sigma_w_a - b[l]\n",
    "            z_list.append(z)\n",
    "\n",
    "        # applying sigmoid function to z\n",
    "        for l in range(len(z_list)):\n",
    "            h = sigmoid_calc(z_list[l])\n",
    "            h_list.append(h)\n",
    "        h_value_list.append(h_list)\n",
    "\n",
    "    # for the number of neuron output have\n",
    "    for j in range(neuron_layer_list[-1]):\n",
    "        # number of input coming from the latest hidden layer to output neuron\n",
    "        # because list[-1] is output, the last hidden layer should be list[-2]\n",
    "        z_output = 0\n",
    "        for k in range(neuron_layer_list[-2]):\n",
    "            w = w_matrix[neuron_layer_list[-2]+j]\n",
    "            z_output = z_output + (h_value_list[-1][k]*w[k])\n",
    "        b = b_matrix[-1]\n",
    "        z_output = z_output - b[j]\n",
    "        y_output = sigmoid_calc(z_output)\n",
    "        y_output_list.append(y_output)\n",
    "    \n",
    "    return y_output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #100. Error = 0.1888935565942776\n",
      "Iteration #200. Error = 0.15956515224932902\n",
      "Iteration #300. Error = 0.15136721664610397\n",
      "Iteration #400. Error = 0.14776616821869368\n",
      "Iteration #500. Error = 0.14578082389305277\n",
      "Iteration #600. Error = 0.1439914022836703\n",
      "Iteration #700. Error = 0.14227615482002587\n",
      "Iteration #800. Error = 0.1406785528230673\n",
      "Iteration #900. Error = 0.13922662169580038\n",
      "Iteration #1000. Error = 0.1379206809404846\n"
     ]
    }
   ],
   "source": [
    "a, b, c, d, e= cost_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.2234852386566955,\n",
       "  -1.4536941112166697,\n",
       "  2.8240658519584927,\n",
       "  3.7637781828568553],\n",
       " [14.114973798542012,\n",
       "  14.138633443833047,\n",
       "  -19.970238789857934,\n",
       "  -23.94671411485086],\n",
       " [-7.15817868621622, -4.937004185084088, 8.48799620126012, 13.0123985507635],\n",
       " [2.490496044203958,\n",
       "  4.18377320293492,\n",
       "  -7.149461729234333,\n",
       "  -3.0680985639808522],\n",
       " [2.672478344282864,\n",
       "  2.7549498015565095,\n",
       "  -4.385836369683323,\n",
       "  -3.671185641220591],\n",
       " [-5.227163592405838,\n",
       "  -3.0336635669894143,\n",
       "  5.511655037505793,\n",
       "  9.00532754466029],\n",
       " [0.73049457892609,\n",
       "  2.9082685718112464,\n",
       "  -3.985364419260196,\n",
       "  -0.9492442465801115],\n",
       " [-2.528006356203929,\n",
       "  -1.4855490965895088,\n",
       "  2.6321989755748327,\n",
       "  3.974306131777282],\n",
       " [-1.7927918921010677,\n",
       "  -6.5274625277606715,\n",
       "  11.506340562933735,\n",
       "  6.099238037114031],\n",
       " [-1.468740260919155,\n",
       "  2.700471079750252,\n",
       "  -2.541209836704162,\n",
       "  2.9050460680207064,\n",
       "  1.773411772241189,\n",
       "  -1.4786108330162113,\n",
       "  1.5501177759946199,\n",
       "  -0.8775719406961736,\n",
       "  -7.1198228267467805],\n",
       " [-1.8271982007884893,\n",
       "  11.27084294341192,\n",
       "  -6.38004533462724,\n",
       "  -5.843338936150002,\n",
       "  -2.829683878975479,\n",
       "  -4.123563831803674,\n",
       "  -3.156994928949043,\n",
       "  -1.8703295606179273,\n",
       "  9.689341460318124],\n",
       " [1.951374518068251,\n",
       "  -9.850284669246554,\n",
       "  3.895472386792506,\n",
       "  -7.013101527569148,\n",
       "  -5.781024161204013,\n",
       "  3.5810709342987908,\n",
       "  -3.8758854993144967,\n",
       "  2.1375296383818596,\n",
       "  7.290589937640951]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.6837584526095415,\n",
       "  0.4498596856506457,\n",
       "  1.3096464155234333,\n",
       "  0.45309461100270204,\n",
       "  0.825204210785079,\n",
       "  1.1930145825228409,\n",
       "  0.5838823731777114,\n",
       "  1.4171085577405178,\n",
       "  0.39245555507713475],\n",
       " [1.5568040940416923, 7.200840928992317, 3.8470241151178395]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5048973136191437,\n",
       " 0.47654950959455816,\n",
       " 0.4712511443206446,\n",
       " 0.4618469340153564,\n",
       " 0.4453163121660445,\n",
       " 0.42073462503409625,\n",
       " 0.3939535461639793,\n",
       " 0.37304782978829304,\n",
       " 0.35966009585635733,\n",
       " 0.3513269441136126,\n",
       " 0.3457715396783159,\n",
       " 0.3416957691486603,\n",
       " 0.3384354757434504,\n",
       " 0.33565273417112784,\n",
       " 0.3331718019265581,\n",
       " 0.33089901486652007,\n",
       " 0.328783353641663,\n",
       " 0.32679633683890785,\n",
       " 0.3249213894916294,\n",
       " 0.32314808082947977,\n",
       " 0.32146897976829064,\n",
       " 0.31987796306885924,\n",
       " 0.31836933954770913,\n",
       " 0.31693742846682565,\n",
       " 0.3155763824592268,\n",
       " 0.3142801336567042,\n",
       " 0.3130423941834051,\n",
       " 0.3118566736581683,\n",
       " 0.3107162952239968,\n",
       " 0.3096144028724058,\n",
       " 0.3085439594047,\n",
       " 0.3074977379659941,\n",
       " 0.3064683116063756,\n",
       " 0.3054480452129476,\n",
       " 0.30442909269770657,\n",
       " 0.30340339993512777,\n",
       " 0.3023627114232643,\n",
       " 0.30129857723844655,\n",
       " 0.3002023579333597,\n",
       " 0.29906522936377156,\n",
       " 0.2978781963981157,\n",
       " 0.29663213175496106,\n",
       " 0.29531786048849157,\n",
       " 0.2939263089757176,\n",
       " 0.29244872852223375,\n",
       " 0.29087698907391013,\n",
       " 0.2892039210215271,\n",
       " 0.2874236667460445,\n",
       " 0.2855319927768448,\n",
       " 0.2835265124229322,\n",
       " 0.28140678033768424,\n",
       " 0.2791742442651653,\n",
       " 0.2768320698281512,\n",
       " 0.2743848819324526,\n",
       " 0.2718384809650622,\n",
       " 0.2691995882114397,\n",
       " 0.2664756557138716,\n",
       " 0.26367475038331384,\n",
       " 0.26080550063749264,\n",
       " 0.25787708201250453,\n",
       " 0.25489921624187367,\n",
       " 0.25188216267948316,\n",
       " 0.24883668749030002,\n",
       " 0.24577400197329954,\n",
       " 0.24270566578614097,\n",
       " 0.23964345405367424,\n",
       " 0.2365991901566666,\n",
       " 0.23358454904089931,\n",
       " 0.23061083931388365,\n",
       " 0.22768877584778324,\n",
       " 0.22482825736717493,\n",
       " 0.22203816477499033,\n",
       " 0.2193261951935996,\n",
       " 0.2166987437630758,\n",
       " 0.21416084056563048,\n",
       " 0.2117161444688516,\n",
       " 0.20936699020426008,\n",
       " 0.20711448051688344,\n",
       " 0.20495861231350546,\n",
       " 0.20289842457958104,\n",
       " 0.2009321562486555,\n",
       " 0.19905740378310954,\n",
       " 0.1972712704659357,\n",
       " 0.1955705018512421,\n",
       " 0.19395160412642,\n",
       " 0.1924109440843894,\n",
       " 0.1909448308941975,\n",
       " 0.18954958088793622,\n",
       " 0.18822156720411343,\n",
       " 0.18695725642239097,\n",
       " 0.18575323437817445,\n",
       " 0.18460622323682996,\n",
       " 0.18351309170156474,\n",
       " 0.1824708599758981,\n",
       " 0.1814767008362859,\n",
       " 0.18052793791566568,\n",
       " 0.1796220420678052,\n",
       " 0.17875662648176746,\n",
       " 0.17792944104735484,\n",
       " 0.17713836633486899,\n",
       " 0.17638140744317357,\n",
       " 0.17565668788528216,\n",
       " 0.1749624436167908,\n",
       " 0.17429701726579036,\n",
       " 0.17365885259014086,\n",
       " 0.17304648916621118,\n",
       " 0.17245855729988813,\n",
       " 0.1718937731436633,\n",
       " 0.17135093400115656,\n",
       " 0.17082891380106446,\n",
       " 0.17032665872506236,\n",
       " 0.169843182977752,\n",
       " 0.1693775646906376,\n",
       " 0.16892894195587585,\n",
       " 0.16849650898883273,\n",
       " 0.16807951242113842,\n",
       " 0.16767724772786582,\n",
       " 0.16728905579366415,\n",
       " 0.1669143196232241,\n",
       " 0.1665524612014202,\n",
       " 0.16620293850796491,\n",
       " 0.16586524269055047,\n",
       " 0.16553889539934905,\n",
       " 0.16522344628448335,\n",
       " 0.16491847065676665,\n",
       " 0.16462356731070193,\n",
       " 0.16433835650748485,\n",
       " 0.16406247811460392,\n",
       " 0.1637955898976116,\n",
       " 0.1635373659587459,\n",
       " 0.16328749531634326,\n",
       " 0.16304568061836858,\n",
       " 0.16281163698292664,\n",
       " 0.16258509095825913,\n",
       " 0.1623657795945105,\n",
       " 0.16215344961940997,\n",
       " 0.16194785670998668,\n",
       " 0.1617487648524848,\n",
       " 0.16155594578276897,\n",
       " 0.1613691784997021,\n",
       " 0.16118824884422905,\n",
       " 0.1610129491371913,\n",
       " 0.1608430778692463,\n",
       " 0.16067843943662768,\n",
       " 0.16051884391689245,\n",
       " 0.16036410687920255,\n",
       " 0.16021404922412694,\n",
       " 0.1600684970483646,\n",
       " 0.15992728153021832,\n",
       " 0.15979023883205684,\n",
       " 0.15965721001639738,\n",
       " 0.1595280409726177,\n",
       " 0.15940258235165672,\n",
       " 0.15928068950639301,\n",
       " 0.15916222243569367,\n",
       " 0.15904704573040407,\n",
       " 0.1589350285198039,\n",
       " 0.15882604441729176,\n",
       " 0.15871997146426825,\n",
       " 0.15861669207138704,\n",
       " 0.15851609295652108,\n",
       " 0.15841806507895217,\n",
       " 0.15832250356944705,\n",
       " 0.15822930765601712,\n",
       " 0.15813838058528895,\n",
       " 0.15804962953952553,\n",
       " 0.1579629655494422,\n",
       " 0.15787830340305622,\n",
       " 0.15779556155088587,\n",
       " 0.1577146620078883,\n",
       " 0.15763553025258098,\n",
       " 0.15755809512383376,\n",
       " 0.15748228871585862,\n",
       " 0.15740804627193947,\n",
       " 0.15733530607745483,\n",
       " 0.1572640093527525,\n",
       " 0.15719410014641233,\n",
       " 0.15712552522942774,\n",
       " 0.15705823399079655,\n",
       " 0.15699217833498563,\n",
       " 0.15692731258168793,\n",
       " 0.1568635933682519,\n",
       " 0.15680097955510647,\n",
       " 0.156739432134465,\n",
       " 0.156678914142531,\n",
       " 0.15661939057538274,\n",
       " 0.15656082830865894,\n",
       " 0.15650319602112478,\n",
       " 0.15644646412214175,\n",
       " 0.15639060468302812,\n",
       " 0.1563355913722517,\n",
       " 0.156281399394362,\n",
       " 0.15622800543253157,\n",
       " 0.15617538759455246,\n",
       " 0.15612352536210178,\n",
       " 0.15607239954307556,\n",
       " 0.15602199222676918,\n",
       " 0.1559722867416727,\n",
       " 0.15592326761563416,\n",
       " 0.15587492053815052,\n",
       " 0.15582723232452983,\n",
       " 0.15578019088168119,\n",
       " 0.15573378517528633,\n",
       " 0.15568800519811757,\n",
       " 0.15564284193927383,\n",
       " 0.15559828735411713,\n",
       " 0.15555433433470967,\n",
       " 0.1555109766805575,\n",
       " 0.15546820906948883,\n",
       " 0.15542602702850844,\n",
       " 0.15538442690448426,\n",
       " 0.15534340583453732,\n",
       " 0.15530296171603059,\n",
       " 0.15526309317605178,\n",
       " 0.15522379954031595,\n",
       " 0.15518508080141769,\n",
       " 0.15514693758637785,\n",
       " 0.1551093711234441,\n",
       " 0.15507238320810984,\n",
       " 0.15503597616833312,\n",
       " 0.15500015282893778,\n",
       " 0.15496491647519298,\n",
       " 0.15493027081556995,\n",
       " 0.154896219943683,\n",
       " 0.1548627682994237,\n",
       " 0.15482992062930775,\n",
       " 0.1547976819460526,\n",
       " 0.15476605748740976,\n",
       " 0.15473505267428345,\n",
       " 0.15470467306816743,\n",
       " 0.15467492432793872,\n",
       " 0.15464581216605275,\n",
       " 0.15461734230418975,\n",
       " 0.15458952042840898,\n",
       " 0.1545623521438777,\n",
       " 0.15453584292924216,\n",
       " 0.1545099980907285,\n",
       " 0.15448482271606123,\n",
       " 0.15446032162830442,\n",
       " 0.15443649933973405,\n",
       " 0.1544133600058721,\n",
       " 0.15439090737981243,\n",
       " 0.1543691447669915,\n",
       " 0.1543480749805604,\n",
       " 0.1543277002975272,\n",
       " 0.1543080224158556,\n",
       " 0.15428904241270666,\n",
       " 0.15427076070402387,\n",
       " 0.15425317700567176,\n",
       " 0.15423629029633534,\n",
       " 0.1542200987824034,\n",
       " 0.15420459986505297,\n",
       " 0.15418979010975525,\n",
       " 0.15417566521842058,\n",
       " 0.15416222000439983,\n",
       " 0.154149448370548,\n",
       " 0.15413734329055007,\n",
       " 0.15412589679369906,\n",
       " 0.15411509995329983,\n",
       " 0.1541049428788582,\n",
       " 0.15409541471219992,\n",
       " 0.1540865036276369,\n",
       " 0.15407819683628554,\n",
       " 0.15407048059460815,\n",
       " 0.1540633402172346,\n",
       " 0.15405676009408512,\n",
       " 0.15405072371179207,\n",
       " 0.15404521367939375,\n",
       " 0.15404021175823568,\n",
       " 0.15403569889599458,\n",
       " 0.15403165526470655,\n",
       " 0.15402806030265354,\n",
       " 0.15402489275993497,\n",
       " 0.15402213074752488,\n",
       " 0.15401975178958235,\n",
       " 0.1540177328787749,\n",
       " 0.15401605053432993,\n",
       " 0.15401468086252773,\n",
       " 0.15401359961931804,\n",
       " 0.154012782274736,\n",
       " 0.15401220407876867,\n",
       " 0.15401184012832134,\n",
       " 0.15401166543491732,\n",
       " 0.15401165499276787,\n",
       " 0.15401178384683256,\n",
       " 0.15401202716050463,\n",
       " 0.15401236028254905,\n",
       " 0.15401275881293156,\n",
       " 0.15401319866718333,\n",
       " 0.15401365613895754,\n",
       " 0.15401410796045148,\n",
       " 0.15401453136037513,\n",
       " 0.1540149041191837,\n",
       " 0.1540152046212896,\n",
       " 0.15401541190401397,\n",
       " 0.15401550570305006,\n",
       " 0.1540154664942426,\n",
       " 0.1540152755315135,\n",
       " 0.15401491488079627,\n",
       " 0.15401436744986574,\n",
       " 0.15401361701398794,\n",
       " 0.15401264823733768,\n",
       " 0.15401144669017072,\n",
       " 0.1540099988617559,\n",
       " 0.1540082921691152,\n",
       " 0.1540063149616378,\n",
       " 0.15400405652166746,\n",
       " 0.15400150706118387,\n",
       " 0.15399865771472865,\n",
       " 0.15399550052873986,\n",
       " 0.1539920284474879,\n",
       " 0.15398823529581634,\n",
       " 0.1539841157589104,\n",
       " 0.1539796653593247,\n",
       " 0.15397488043151794,\n",
       " 0.15396975809414,\n",
       " 0.15396429622033336,\n",
       " 0.153958493406302,\n",
       " 0.15395234893840995,\n",
       " 0.1539458627590636,\n",
       " 0.1539390354316286,\n",
       " 0.15393186810462667,\n",
       " 0.15392436247544958,\n",
       " 0.15391652075381493,\n",
       " 0.15390834562517944,\n",
       " 0.1538998402143115,\n",
       " 0.1538910080492124,\n",
       " 0.1538818530255592,\n",
       " 0.15387237937182777,\n",
       " 0.15386259161524224,\n",
       " 0.15385249454867614,\n",
       " 0.15384209319861872,\n",
       " 0.1538313927943047,\n",
       " 0.15382039873808873,\n",
       " 0.15380911657713175,\n",
       " 0.15379755197645428,\n",
       " 0.15378571069339655,\n",
       " 0.1537735985535134,\n",
       " 0.1537612214279199,\n",
       " 0.15374858521209447,\n",
       " 0.1537356958061336,\n",
       " 0.15372255909644672,\n",
       " 0.15370918093886757,\n",
       " 0.1536955671431553,\n",
       " 0.15368172345885253,\n",
       " 0.15366765556245454,\n",
       " 0.15365336904585253,\n",
       " 0.15363886940599783,\n",
       " 0.15362416203573653,\n",
       " 0.1536092522157651,\n",
       " 0.15359414510764746,\n",
       " 0.15357884574783864,\n",
       " 0.15356335904266316,\n",
       " 0.1535476897641844,\n",
       " 0.15353184254691613,\n",
       " 0.15351582188531498,\n",
       " 0.1534996321320067,\n",
       " 0.15348327749668572,\n",
       " 0.15346676204564505,\n",
       " 0.15345008970188023,\n",
       " 0.15343326424572626,\n",
       " 0.15341628931597812,\n",
       " 0.15339916841145404,\n",
       " 0.15338190489296033,\n",
       " 0.15336450198561935,\n",
       " 0.15334696278152338,\n",
       " 0.1533292902426826,\n",
       " 0.15331148720423196,\n",
       " 0.15329355637787082,\n",
       " 0.1532755003555042,\n",
       " 0.15325732161306233,\n",
       " 0.15323902251447355,\n",
       " 0.15322060531576803,\n",
       " 0.15320207216929296,\n",
       " 0.15318342512802016,\n",
       " 0.1531646661499288,\n",
       " 0.1531457971024461,\n",
       " 0.15312681976693465,\n",
       " 0.1531077358432097,\n",
       " 0.1530885469540769,\n",
       " 0.1530692546498771,\n",
       " 0.15304986041303048,\n",
       " 0.15303036566256886,\n",
       " 0.15301077175864974,\n",
       " 0.15299108000704081,\n",
       " 0.1529712916635726,\n",
       " 0.1529514079385497,\n",
       " 0.15293143000111487,\n",
       " 0.15291135898356153,\n",
       " 0.1528911959855894,\n",
       " 0.15287094207849844,\n",
       " 0.15285059830931633,\n",
       " 0.15283016570485608,\n",
       " 0.15280964527570168,\n",
       " 0.1527890380201137,\n",
       " 0.15276834492785632,\n",
       " 0.1527475669839408,\n",
       " 0.1527267051722824,\n",
       " 0.15270576047926737,\n",
       " 0.15268473389722842,\n",
       " 0.1526636264278276,\n",
       " 0.15264243908533998,\n",
       " 0.1526211728998391,\n",
       " 0.15259982892028412,\n",
       " 0.15257840821749852,\n",
       " 0.1525569118870497,\n",
       " 0.15253534105201721,\n",
       " 0.15251369686565525,\n",
       " 0.15249198051394355,\n",
       " 0.15247019321802793,\n",
       " 0.15244833623654608,\n",
       " 0.15242641086784178,\n",
       " 0.15240441845206118,\n",
       " 0.15238236037313482,\n",
       " 0.1523602380606419,\n",
       " 0.1523380529915564,\n",
       " 0.15231580669187583,\n",
       " 0.15229350073813092,\n",
       " 0.15227113675877585,\n",
       " 0.15224871643546056,\n",
       " 0.1522262415041849,\n",
       " 0.15220371375633207,\n",
       " 0.15218113503958564,\n",
       " 0.15215850725872945,\n",
       " 0.15213583237633058,\n",
       " 0.15211311241330622,\n",
       " 0.15209034944937733,\n",
       " 0.15206754562340916,\n",
       " 0.1520447031336393,\n",
       " 0.15202182423779786,\n",
       " 0.15199891125311898,\n",
       " 0.1519759665562464,\n",
       " 0.15195299258303685,\n",
       " 0.15192999182826156,\n",
       " 0.15190696684521043,\n",
       " 0.15188392024519928,\n",
       " 0.151860854696986,\n",
       " 0.15183777292609602,\n",
       " 0.1518146777140622,\n",
       " 0.15179157189758083,\n",
       " 0.15176845836758884,\n",
       " 0.15174534006826404,\n",
       " 0.15172221999595348,\n",
       " 0.15169910119803284,\n",
       " 0.1516759867717011,\n",
       " 0.15165287986271334,\n",
       " 0.1516297836640586,\n",
       " 0.15160670141458127,\n",
       " 0.1515836363975566,\n",
       " 0.15156059193921945,\n",
       " 0.15153757140725238,\n",
       " 0.15151457820923775,\n",
       " 0.1514916157910755,\n",
       " 0.15146868763537327,\n",
       " 0.15144579725981208,\n",
       " 0.15142294821549113,\n",
       " 0.15140014408525415,\n",
       " 0.1513773884820069,\n",
       " 0.15135468504702015,\n",
       " 0.1513320374482335,\n",
       " 0.15130944937855217,\n",
       " 0.15128692455414877,\n",
       " 0.1512644667127708,\n",
       " 0.15124207961205416,\n",
       " 0.15121976702785142,\n",
       " 0.15119753275257392,\n",
       " 0.15117538059355137,\n",
       " 0.15115331437141355,\n",
       " 0.15113133791849478,\n",
       " 0.15110945507726392,\n",
       " 0.15108766969878387,\n",
       " 0.15106598564120047,\n",
       " 0.1510444067682644,\n",
       " 0.15102293694788727,\n",
       " 0.15100158005073422,\n",
       " 0.15098033994885346,\n",
       " 0.15095922051434474,\n",
       " 0.15093822561806727,\n",
       " 0.1509173591283882,\n",
       " 0.15089662490997507,\n",
       " 0.15087602682262535,\n",
       " 0.15085556872014272,\n",
       " 0.1508352544492529,\n",
       " 0.15081508784856235,\n",
       " 0.1507950727475594,\n",
       " 0.1507752129656548,\n",
       " 0.1507555123112642,\n",
       " 0.15073597458092886,\n",
       " 0.15071660355847522,\n",
       " 0.15069740301420984,\n",
       " 0.15067837670414888,\n",
       " 0.15065952836928267,\n",
       " 0.15064086173486627,\n",
       " 0.15062238050974158,\n",
       " 0.15060408838568112,\n",
       " 0.1505859890367564,\n",
       " 0.1505680861187213,\n",
       " 0.15055038326841125,\n",
       " 0.15053288410315246,\n",
       " 0.15051559222017533,\n",
       " 0.1504985111960301,\n",
       " 0.15048164458599453,\n",
       " 0.15046499592347515,\n",
       " 0.1504485687193871,\n",
       " 0.15043236646151226,\n",
       " 0.15041639261382697,\n",
       " 0.1504006506157887,\n",
       " 0.1503851438815765,\n",
       " 0.15036987579927605,\n",
       " 0.1503548497299949,\n",
       " 0.15034006900690605,\n",
       " 0.15032553693419862,\n",
       " 0.15031125678593182,\n",
       " 0.15029723180477506,\n",
       " 0.15028346520061991,\n",
       " 0.15026996014905325,\n",
       " 0.15025671978967065,\n",
       " 0.15024374722421596,\n",
       " 0.15023104551452945,\n",
       " 0.15021861768028189,\n",
       " 0.15020646669647938,\n",
       " 0.15019459549071132,\n",
       " 0.15018300694012113,\n",
       " 0.15017170386807543,\n",
       " 0.1501606890405052,\n",
       " 0.15014996516188817,\n",
       " 0.15013953487084827,\n",
       " 0.15012940073533698,\n",
       " 0.15011956524736636,\n",
       " 0.1501100308172604,\n",
       " 0.1501007997673861,\n",
       " 0.15009187432533252,\n",
       " 0.15008325661649496,\n",
       " 0.15007494865602827,\n",
       " 0.15006695234012812,\n",
       " 0.15005926943660003,\n",
       " 0.15005190157467418,\n",
       " 0.15004485023402708,\n",
       " 0.1500381167329673,\n",
       " 0.15003170221574844,\n",
       " 0.15002560763897113,\n",
       " 0.15001983375703795,\n",
       " 0.1500143811066343,\n",
       " 0.15000924999020337,\n",
       " 0.1500044404583999,\n",
       " 0.14999995229150667,\n",
       " 0.14999578497980967,\n",
       " 0.1499919377029414,\n",
       " 0.1499884093082076,\n",
       " 0.14998519828793272,\n",
       " 0.14998230275587943,\n",
       " 0.14997972042280555,\n",
       " 0.14997744857125983,\n",
       " 0.14997548402972996,\n",
       " 0.14997382314629298,\n",
       " 0.14997246176194315,\n",
       " 0.14997139518381472,\n",
       " 0.14997061815854348,\n",
       " 0.14997012484606057,\n",
       " 0.14996990879414712,\n",
       " 0.14996996291412523,\n",
       " 0.14997027945810415,\n",
       " 0.14997084999824742,\n",
       " 0.1499716654085672,\n",
       " 0.1499727158497976,\n",
       " 0.14997399075793386,\n",
       " 0.14997547883705928,\n",
       " 0.14997716805710437,\n",
       " 0.14997904565719863,\n",
       " 0.1499810981552775,\n",
       " 0.14998331136459503,\n",
       " 0.14998567041776525,\n",
       " 0.14998815979890104,\n",
       " 0.1499907633843535,\n",
       " 0.14999346449245898,\n",
       " 0.14999624594257838,\n",
       " 0.14999909012357454,\n",
       " 0.15000197907169696,\n",
       " 0.15000489455765684,\n",
       " 0.1500078181824622,\n",
       " 0.15001073148134764,\n",
       " 0.15001361603490537,\n",
       " 0.15001645358627022,\n",
       " 0.15001922616298413,\n",
       " 0.15002191620193717,\n",
       " 0.15002450667558265,\n",
       " 0.15002698121746402,\n",
       " 0.15002932424496632,\n",
       " 0.15003152107713635,\n",
       " 0.15003355804541088,\n",
       " 0.15003542259514782,\n",
       " 0.15003710337597592,\n",
       " 0.15003859031917471,\n",
       " 0.1500398747005456,\n",
       " 0.15004094918754654,\n",
       " 0.15004180786981827,\n",
       " 0.15004244627261168,\n",
       " 0.15004286135304218,\n",
       " 0.15004305147949143,\n",
       " 0.15004301639487905,\n",
       " 0.1500427571648896,\n",
       " 0.15004227611255871,\n",
       " 0.15004157674089177,\n",
       " 0.1500406636453934,\n",
       " 0.15003954241852102,\n",
       " 0.1500382195481487,\n",
       " 0.15003670231212402,\n",
       " 0.15003499867094186,\n",
       " 0.1500331171604457,\n",
       " 0.15003106678630285,\n",
       " 0.15002885692180282,\n",
       " 0.15002649721030967,\n",
       " 0.15002399747346032,\n",
       " 0.1500213676259658,\n",
       " 0.15001861759763463,\n",
       " 0.15001575726301564,\n",
       " 0.1500127963788564,\n",
       " 0.1500097445293881,\n",
       " 0.15000661107929536,\n",
       " 0.15000340513409482,\n",
       " 0.15000013550754343,\n",
       " 0.14999681069562165,\n",
       " 0.14999343885657554,\n",
       " 0.14999002779647178,\n",
       " 0.14998658495970058,\n",
       " 0.1499831174238623,\n",
       " 0.14997963189848634,\n",
       " 0.14997613472704963,\n",
       " 0.14997263189179738,\n",
       " 0.14996912902089743,\n",
       " 0.14996563139750643,\n",
       " 0.14996214397035418,\n",
       " 0.14995867136550764,\n",
       " 0.14995521789900368,\n",
       " 0.14995178759008515,\n",
       " 0.14994838417480857,\n",
       " 0.14994501111982614,\n",
       " 0.14994167163617644,\n",
       " 0.1499383686929437,\n",
       " 0.1499351050306741,\n",
       " 0.14993188317445932,\n",
       " 0.14992870544661255,\n",
       " 0.14992557397888862,\n",
       " 0.1499224907242034,\n",
       " 0.1499194574678296,\n",
       " 0.14991647583805157,\n",
       " 0.14991354731627188,\n",
       " 0.14991067324656912,\n",
       " 0.1499078548447153,\n",
       " 0.14990509320666007,\n",
       " 0.1499023893165,\n",
       " 0.14989974405394568,\n",
       " 0.14989715820131314,\n",
       " 0.1498946324500509,\n",
       " 0.14989216740683378,\n",
       " 0.1498897635992404,\n",
       " 0.14988742148103795,\n",
       " 0.14988514143709752,\n",
       " 0.14988292378796156,\n",
       " 0.14988076879408482,\n",
       " 0.1498786766597699,\n",
       " 0.1498766475368169,\n",
       " 0.1498746815279073,\n",
       " 0.14987277868973936,\n",
       " 0.14987093903593438,\n",
       " 0.14986916253972607,\n",
       " 0.14986744913645447,\n",
       " 0.14986579872587355,\n",
       " 0.1498642111742901,\n",
       " 0.1498626863165441,\n",
       " 0.1498612239578441,\n",
       " 0.14985982387546812,\n",
       " 0.1498584858203414,\n",
       " 0.14985720951849876,\n",
       " 0.14985599467244282,\n",
       " 0.149854840962405,\n",
       " 0.14985374804751717,\n",
       " 0.14985271556690216,\n",
       " 0.1498517431406879,\n",
       " 0.14985083037095256,\n",
       " 0.14984997684260787,\n",
       " 0.1498491821242206,\n",
       " 0.14984844576878406,\n",
       " 0.1498477673144383,\n",
       " 0.1498471462851455,\n",
       " 0.1498465821913252,\n",
       " 0.1498460745304505,\n",
       " 0.14984562278760927,\n",
       " 0.1498452264360356,\n",
       " 0.14984488493760986,\n",
       " 0.1498445977433335,\n",
       " 0.14984436429377868,\n",
       " 0.1498441840195167,\n",
       " 0.149844056341525,\n",
       " 0.1498439806715763,\n",
       " 0.14984395641260984,\n",
       " 0.14984398295908788,\n",
       " 0.14984405969733786,\n",
       " 0.14984418600588167,\n",
       " 0.14984436125575246,\n",
       " 0.14984458481080107,\n",
       " 0.14984485602799316,\n",
       " 0.14984517425769683,\n",
       " 0.1498455388439621,\n",
       " 0.14984594912479207,\n",
       " 0.14984640443241054,\n",
       " 0.14984690409351833,\n",
       " 0.14984744742954922,\n",
       " 0.14984803375691672,\n",
       " 0.14984866238725927,\n",
       " 0.1498493326276788,\n",
       " 0.1498500437809766,\n",
       " 0.14985079514588662,\n",
       " 0.14985158601730256,\n",
       " 0.14985241568650662,\n",
       " 0.14985328344139048,\n",
       " 0.1498541885666783,\n",
       " 0.1498551303441445,\n",
       " 0.14985610805283123,\n",
       " 0.14985712096926304,\n",
       " 0.1498581683676602,\n",
       " 0.14985924952015067,\n",
       " 0.14986036369698022,\n",
       " 0.14986151016672034,\n",
       " 0.1498626881964775,\n",
       " 0.14986389705209754,\n",
       " 0.14986513599837237,\n",
       " 0.14986640429924214,\n",
       " 0.14986770121799955,\n",
       " 0.14986902601749102,\n",
       " 0.14987037796031713,\n",
       " 0.14987175630903196,\n",
       " 0.14987316032634268,\n",
       " 0.14987458927530586,\n",
       " 0.14987604241952387,\n",
       " 0.14987751902334073,\n",
       " 0.1498790183520358,\n",
       " 0.1498805396720162,\n",
       " 0.14988208225100927,\n",
       " 0.14988364535825202,\n",
       " 0.14988522826468179,\n",
       " 0.14988683024312155,\n",
       " 0.14988845056846792,\n",
       " 0.14989008851787577,\n",
       " 0.149891743370941,\n",
       " 0.14989341440988205,\n",
       " 0.14989510091972036,\n",
       " 0.14989680218845838,\n",
       " 0.14989851750725544,\n",
       " 0.14990024617060335,\n",
       " 0.1499019874764978,\n",
       " 0.14990374072660903,\n",
       " 0.14990550522645096,\n",
       " 0.1499072802855469,\n",
       " 0.14990906521759234,\n",
       " 0.14991085934061785,\n",
       " 0.14991266197714845,\n",
       " 0.1499144724543582,\n",
       " 0.14991629010422652,\n",
       " 0.14991811426368748,\n",
       " 0.14991994427477978,\n",
       " 0.1499217794847918,\n",
       " 0.1499236192464044,\n",
       " 0.14992546291783146,\n",
       " 0.14992730986295552,\n",
       " 0.14992915945146268,\n",
       " 0.14993101105897277,\n",
       " 0.14993286406716622,\n",
       " 0.14993471786390825,\n",
       " 0.14993657184336978,\n",
       " 0.14993842540614452,\n",
       " 0.14994027795936224,\n",
       " 0.14994212891679898,\n",
       " 0.14994397769898446,\n",
       " 0.14994582373330353,\n",
       " 0.14994766645409696,\n",
       " 0.1499495053027551,\n",
       " 0.14995133972781113,\n",
       " 0.14995316918502827,\n",
       " 0.14995499313748362,\n",
       " 0.14995681105564904,\n",
       " 0.1499586224174658,\n",
       " 0.14996042670841928,\n",
       " 0.14996222342160478,\n",
       " 0.14996401205779433,\n",
       " 0.149965792125495,\n",
       " 0.14996756314100632,\n",
       " 0.14996932462847257,\n",
       " 0.14997107611993182,\n",
       " 0.14997281715535923,\n",
       " 0.14997454728270807,\n",
       " 0.1499762660579464,\n",
       " 0.1499779730450891,\n",
       " 0.149979667816227,\n",
       " 0.14998134995155066,\n",
       " 0.14998301903937145,\n",
       " 0.14998467467613907,\n",
       " 0.1499863164664532,\n",
       " 0.14998794402307364,\n",
       " 0.1499895569669248,\n",
       " 0.14999115492709864,\n",
       " 0.14999273754085055,\n",
       " 0.14999430445359552,\n",
       " 0.14999585531889748,\n",
       " 0.1499973897984569,\n",
       " 0.14999890756209444,\n",
       " 0.15000040828773004,\n",
       " 0.15000189166136096,\n",
       " 0.15000335737703419,\n",
       " 0.15000480513681672,\n",
       " 0.15000623465076274,\n",
       " 0.15000764563687694,\n",
       " 0.1500090378210753,\n",
       " 0.15001041093714318,\n",
       " 0.15001176472668923,\n",
       " 0.15001309893909806,\n",
       " 0.15001441333147889,\n",
       " 0.15001570766861202,\n",
       " 0.15001698172289346,\n",
       " 0.15001823527427452,\n",
       " 0.15001946811020242,\n",
       " 0.150020680025556,\n",
       " 0.1500218708225793,\n",
       " 0.15002304031081456,\n",
       " 0.15002418830703138,\n",
       " 0.15002531463515545,\n",
       " 0.15002641912619222,\n",
       " 0.15002750161815417,\n",
       " 0.15002856195598,\n",
       " 0.1500295999914568,\n",
       " 0.15003061558313896,\n",
       " 0.15003160859626416,\n",
       " 0.1500325789026714,\n",
       " 0.15003352638071346,\n",
       " 0.15003445091517206,\n",
       " 0.15003535239716823,\n",
       " 0.15003623072407485,\n",
       " 0.15003708579942546,\n",
       " 0.15003791753282397,\n",
       " 0.1500387258398528,\n",
       " 0.150039510641979,\n",
       " 0.15004027186646332,\n",
       " 0.15004100944626314,\n",
       " 0.15004172331994056,\n",
       " 0.15004241343156632,\n",
       " 0.15004307973062328,\n",
       " 0.1500437221719126,\n",
       " 0.15004434071545564,\n",
       " 0.15004493532639926,\n",
       " 0.15004550597491825,\n",
       " 0.15004605263611925,\n",
       " 0.150046575289944,\n",
       " 0.1500470739210728,\n",
       " 0.15004754851882868,\n",
       " 0.15004799907707916,\n",
       " 0.15004842559414297,\n",
       " 0.15004882807269254,\n",
       " 0.15004920651965845,\n",
       " 0.15004956094613503,\n",
       " 0.15004989136728575,\n",
       " 0.15005019780224774,\n",
       " 0.15005048027404042,\n",
       " 0.15005073880946954,\n",
       " 0.15005097343903742,\n",
       " 0.15005118419684907,\n",
       " 0.15005137112052178,\n",
       " 0.15005153425109516,\n",
       " 0.1500516736329404,\n",
       " 0.15005178931367172,\n",
       " 0.15005188134405928,\n",
       " 0.15005194977794029,\n",
       " 0.1500519946721335,\n",
       " 0.15005201608635424,\n",
       " 0.15005201408312885,\n",
       " 0.15005198872771208,\n",
       " 0.1500519400880037,\n",
       " 0.1500518682344679,\n",
       " 0.15005177324005192,\n",
       " 0.15005165518010638,\n",
       " 0.15005151413230827,\n",
       " 0.15005135017658214,\n",
       " 0.15005116339502433,\n",
       " 0.15005095387182782,\n",
       " 0.15005072169320793,\n",
       " 0.15005046694733007,\n",
       " 0.15005018972423728,\n",
       " 0.1500498901157795,\n",
       " 0.15004956821554472,\n",
       " 0.1500492241187899,\n",
       " 0.15004885792237455,\n",
       " 0.1500484697246932,\n",
       " 0.15004805962561293,\n",
       " 0.1500476277264069,\n",
       " 0.15004717412969387,\n",
       " 0.1500466989393759,\n",
       " 0.15004620226057855,\n",
       " 0.15004568419959105,\n",
       " 0.15004514486380932,\n",
       " 0.15004458436167864,\n",
       " 0.15004400280263844,\n",
       " 0.15004340029706695,\n",
       " 0.15004277695622917,\n",
       " 0.15004213289222412,\n",
       " 0.15004146821793302,\n",
       " 0.15004078304697094,\n",
       " 0.15004007749363601,\n",
       " 0.15003935167286417,\n",
       " 0.15003860570017957,\n",
       " 0.15003783969165188,\n",
       " 0.15003705376384957,\n",
       " 0.15003624803379811,\n",
       " 0.1500354226189364,\n",
       " 0.15003457763707662,\n",
       " 0.15003371320636236,\n",
       " 0.15003282944523075,\n",
       " 0.1500319264723746,\n",
       " 0.15003100440670258,\n",
       " 0.15003006336730562,\n",
       " 0.15002910347342052,\n",
       " 0.1500281248443954,\n",
       " 0.1500271275996564,\n",
       " 0.1500261118586752,\n",
       " 0.15002507774093793,\n",
       " 0.15002402536591355,\n",
       " 0.15002295485302478,\n",
       " 0.1500218663216188,\n",
       " 0.1500207598909392,\n",
       " 0.15001963568009896,\n",
       " 0.1500184938080544,\n",
       " 0.15001733439357837,\n",
       " 0.1500161575552365,\n",
       " 0.1500149634113634,\n",
       " 0.15001375208003845,\n",
       " 0.15001252367906429,\n",
       " 0.15001127832594455,\n",
       " 0.15001001613786358,\n",
       " 0.15000873723166472,\n",
       " 0.1500074417238328,\n",
       " 0.15000612973047364,\n",
       " 0.15000480136729646,\n",
       " 0.15000345674959695,\n",
       " 0.15000209599223865,\n",
       " 0.15000071920963923,\n",
       " 0.1499993265157525,\n",
       " 0.1499979180240554,\n",
       " 0.14999649384753183,\n",
       " 0.14999505409866015,\n",
       " 0.14999359888939912,\n",
       " 0.14999212833117595,\n",
       " 0.1499906425348739,\n",
       " 0.14998914161082,\n",
       " 0.14998762566877455,\n",
       " 0.1499860948179211,\n",
       " 0.1499845491668553,\n",
       " 0.14998298882357544,\n",
       " 0.14998141389547476,\n",
       " 0.14997982448933062,\n",
       " 0.14997822071129777,\n",
       " 0.14997660266690024,\n",
       " 0.14997497046102373,\n",
       " 0.14997332419790926,\n",
       " 0.14997166398114545,\n",
       " 0.14996998991366414,\n",
       " 0.14996830209773296,\n",
       " 0.14996660063495118,\n",
       " 0.14996488562624394,\n",
       " 0.14996315717185785,\n",
       " 0.14996141537135732,\n",
       " 0.14995966032361885,\n",
       " 0.14995789212682983,\n",
       " 0.14995611087848296,\n",
       " 0.14995431667537473,\n",
       " 0.14995250961360165,\n",
       " 0.1499506897885589,\n",
       " 0.14994885729493734,\n",
       " 0.1499470122267216,\n",
       " 0.1499451546771895,\n",
       " 0.14994328473890928,\n",
       " 0.14994140250373955,\n",
       " 0.14993950806282796,\n",
       " 0.14993760150661095,\n",
       " 0.14993568292481296,\n",
       " 0.149933752406446,\n",
       " 0.1499318100398104,\n",
       " 0.14992985591249486,\n",
       " 0.14992789011137667,\n",
       " 0.1499259127226221,\n",
       " 0.1499239238316879,\n",
       " 0.14992192352332187,\n",
       " 0.14991991188156403,\n",
       " 0.1499178889897477,\n",
       " 0.14991585493050183,\n",
       " 0.14991380978575194,\n",
       " 0.1499117536367218,\n",
       " 0.14990968656393597,\n",
       " 0.14990760864722164,\n",
       " 0.14990551996571064,\n",
       " 0.149903420597842,\n",
       " 0.14990131062136436,\n",
       " 0.1498991901133386]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14a40f0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1352a410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title('RMSE value per iteration', size = 25)\n",
    "plt.xlabel('Iteration', size = 16)\n",
    "plt.ylabel('RMSE', size = 16)\n",
    "plt.text(400, .35, r'$\\eta=%s \\ | \\ 1 \\ Hidden \\ Layer \\ (%s \\ node)$' %(learning_rate, neuron_layer_list[1]), size = 15) \n",
    "plt.plot(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 21.629574298858643 seconds \n"
     ]
    }
   ],
   "source": [
    "print(\"Running time: %s seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output_list = []\n",
    "for i in range(len(d)):\n",
    "    d2 = d[i]\n",
    "    d3 = d2[4:]\n",
    "    expected_output_list.append(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = []\n",
    "for i in range(len(d)):\n",
    "    test = recall(d[i], a, b)\n",
    "    for j in range(len(test)):\n",
    "        if test[j] >= 0.5:\n",
    "            test[j] = 1\n",
    "        else:\n",
    "            test[j] = 0\n",
    "    test_output.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_true = []\n",
    "for i in range(len(d)):\n",
    "    a = test_output[i] == expected_output_list[i]\n",
    "    list_true.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#30 test training from the dataset\n",
    "sum(list_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## need to upgrade bunch of loop to allow automated calculation of any number of nodes and hidden layer\n",
    "### initiate random upgrade (can use dataset parameter), using 2(i) + 1 number of nodes (i) is number of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read iris dataset csv using pandas\n",
    "data_matrix = pd.read_csv(\"Iris_Dataset.csv\")\n",
    "# create output neuron\n",
    "data_matrix['o1'] = 0\n",
    "data_matrix['o2'] = 0\n",
    "data_matrix['o3'] = 0\n",
    "# set output value. iris-setosa[1,0,0], iris-versicolor[0,1,0], iris-virginica[0,0,1]\n",
    "data_matrix.loc[data_matrix['Output'] == 'Iris-setosa', 'o1'] = 1\n",
    "data_matrix.loc[data_matrix['Output'] == 'Iris-versicolor', 'o2'] = 1\n",
    "data_matrix.loc[data_matrix['Output'] == 'Iris-virginica', 'o3'] = 1\n",
    "# shuffle dataset\n",
    "data_matrix = data_matrix.sample(frac=1).reset_index(drop=True)\n",
    "# delete output column\n",
    "del data_matrix['Output']\n",
    "# convert dataframe to list\n",
    "data_list = data_matrix.values.tolist()\n",
    "# normalize\n",
    "data_list2 = np.array(data_list)\n",
    "data_list3 = normalize(data_list2[:,0:4]).tolist()\n",
    "data_list4 = data_list2[:,4:].tolist()\n",
    "data_list5 = []\n",
    "for i in range(len(data_list3)):\n",
    "    data_list3[i].extend(data_list4[i])\n",
    "    data_list5.append(data_list3[i])\n",
    "data_list6 = data_list5[0:120].copy()\n",
    "data_list7 = data_list5[120:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>o1</th>\n",
       "      <th>o2</th>\n",
       "      <th>o3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       A    B    C    D  o1  o2  o3\n",
       "0    7.6  3.0  6.6  2.1   0   0   1\n",
       "1    5.1  3.7  1.5  0.4   1   0   0\n",
       "2    7.7  2.6  6.9  2.3   0   0   1\n",
       "3    5.7  3.8  1.7  0.3   1   0   0\n",
       "4    5.8  2.7  5.1  1.9   0   0   1\n",
       "5    6.1  2.8  4.0  1.3   0   1   0\n",
       "6    5.2  4.1  1.5  0.1   1   0   0\n",
       "7    6.6  3.0  4.4  1.4   0   1   0\n",
       "8    5.7  3.0  4.2  1.2   0   1   0\n",
       "9    5.5  2.4  3.7  1.0   0   1   0\n",
       "10   6.4  2.8  5.6  2.1   0   0   1\n",
       "11   5.4  3.0  4.5  1.5   0   1   0\n",
       "12   6.6  2.9  4.6  1.3   0   1   0\n",
       "13   6.3  3.3  6.0  2.5   0   0   1\n",
       "14   6.4  2.9  4.3  1.3   0   1   0\n",
       "15   4.4  3.0  1.3  0.2   1   0   0\n",
       "16   6.4  2.8  5.6  2.2   0   0   1\n",
       "17   4.6  3.2  1.4  0.2   1   0   0\n",
       "18   6.3  3.3  4.7  1.6   0   1   0\n",
       "19   5.2  3.5  1.5  0.2   1   0   0\n",
       "20   5.8  2.7  4.1  1.0   0   1   0\n",
       "21   5.6  2.5  3.9  1.1   0   1   0\n",
       "22   5.9  3.0  5.1  1.8   0   0   1\n",
       "23   6.2  2.2  4.5  1.5   0   1   0\n",
       "24   6.4  3.2  5.3  2.3   0   0   1\n",
       "25   5.7  4.4  1.5  0.4   1   0   0\n",
       "26   7.4  2.8  6.1  1.9   0   0   1\n",
       "27   6.2  2.9  4.3  1.3   0   1   0\n",
       "28   7.7  2.8  6.7  2.0   0   0   1\n",
       "29   5.9  3.2  4.8  1.8   0   1   0\n",
       "..   ...  ...  ...  ...  ..  ..  ..\n",
       "120  6.7  2.5  5.8  1.8   0   0   1\n",
       "121  6.3  2.8  5.1  1.5   0   0   1\n",
       "122  6.1  3.0  4.9  1.8   0   0   1\n",
       "123  6.3  2.5  5.0  1.9   0   0   1\n",
       "124  4.9  2.4  3.3  1.0   0   1   0\n",
       "125  7.3  2.9  6.3  1.8   0   0   1\n",
       "126  6.5  3.0  5.5  1.8   0   0   1\n",
       "127  5.2  2.7  3.9  1.4   0   1   0\n",
       "128  6.5  2.8  4.6  1.5   0   1   0\n",
       "129  6.3  2.7  4.9  1.8   0   0   1\n",
       "130  4.9  3.0  1.4  0.2   1   0   0\n",
       "131  6.7  3.1  4.4  1.4   0   1   0\n",
       "132  4.8  3.4  1.9  0.2   1   0   0\n",
       "133  5.4  3.4  1.5  0.4   1   0   0\n",
       "134  5.5  2.3  4.0  1.3   0   1   0\n",
       "135  7.7  3.8  6.7  2.2   0   0   1\n",
       "136  5.0  3.5  1.6  0.6   1   0   0\n",
       "137  5.7  2.8  4.1  1.3   0   1   0\n",
       "138  6.3  3.4  5.6  2.4   0   0   1\n",
       "139  5.0  2.3  3.3  1.0   0   1   0\n",
       "140  5.5  4.2  1.4  0.2   1   0   0\n",
       "141  6.0  2.9  4.5  1.5   0   1   0\n",
       "142  5.4  3.9  1.3  0.4   1   0   0\n",
       "143  6.1  2.9  4.7  1.4   0   1   0\n",
       "144  7.7  3.0  6.1  2.3   0   0   1\n",
       "145  4.5  2.3  1.3  0.3   1   0   0\n",
       "146  6.7  3.3  5.7  2.1   0   0   1\n",
       "147  6.7  3.0  5.2  2.3   0   0   1\n",
       "148  6.9  3.1  4.9  1.5   0   1   0\n",
       "149  6.8  3.2  5.9  2.3   0   0   1\n",
       "\n",
       "[150 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
